{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.autonotebook import tqdm, trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: C:\\ProgramData\\sagemaker\\sagemaker\\config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: C:\\Users\\souga\\AppData\\Local\\sagemaker\\sagemaker\\config.yaml\n"
     ]
    }
   ],
   "source": [
    "## functional dependencies\n",
    "import time\n",
    "\n",
    "## setting up env\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from numpy.core.defchararray import endswith\n",
    "load_dotenv()\n",
    "\n",
    "## LangChain dependencies\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_chroma import Chroma\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_cohere.chat_models import ChatCohere\n",
    "## LCEL implementation of LangChain ConversationalRetrievalChain\n",
    "from langchain.chains import create_history_aware_retriever, create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# For both .py and .ipynb files\n",
    "try:\n",
    "    # For .py files\n",
    "    current_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "except NameError:\n",
    "    # For .ipynb files\n",
    "    current_dir = os.getcwd()\n",
    "\n",
    "data_path = os.path.join(current_dir, \"data\")\n",
    "persistent_directory = os.path.join(current_dir, \"data-ingestion-local\")\n",
    "\n",
    "\n",
    "llm = ChatGroq(model=\"llama-3.1-8b-instant\", temperature=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## open-source embedding model from HuggingFace - taking the default model only\n",
    "embedF = HuggingFaceEmbeddings(model_name = \"all-MiniLM-L6-v2\")\n",
    "\n",
    "## loading the vector database from local\n",
    "vectorDB = Chroma(embedding_function=embedF, persist_directory=persistent_directory)\n",
    "\n",
    "## setting up the retriever\n",
    "kb_retriever = vectorDB.as_retriever(search_type=\"similarity\",search_kwargs={\"k\": 3})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "contextualize_q_system_prompt = (\n",
    "    \"\"\"\n",
    "        TASK: Convert context-dependent questions into standalone queries.\n",
    "\n",
    "        INPUT: \n",
    "        - chat_history: Previous messages\n",
    "        - question: Current user query\n",
    "\n",
    "        RULES:\n",
    "        1. Replace pronouns (it/they/this) with specific referents\n",
    "        2. Expand contextual phrases (\"the above\", \"previous\")\n",
    "        3. Return original if already standalone\n",
    "        4. NEVER answer or explain - only reformulate\n",
    "\n",
    "        OUTPUT: Single reformulated question, preserving original intent and style.\n",
    "\n",
    "        Example:\n",
    "        History: \"Let's discuss Python.\"\n",
    "        Question: \"How do I use it?\"\n",
    "        Returns: \"How do I use Python?\"\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualize_q_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "history_aware_retriever = create_history_aware_retriever(\n",
    "    llm = llm,\n",
    "    retriever = kb_retriever,\n",
    "    prompt = contextualize_q_prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## setting-up the prompt\n",
    "system_prompt_template = (\n",
    "    \"As a Legal Assistant Chatbot specializing in legal queries, \"\n",
    "    \"your primary objective is to provide accurate and concise information based on user queries. \"\n",
    "    \"You will adhere strictly to the instructions provided, offering relevant \"\n",
    "    \"context from the knowledge base while avoiding unnecessary details. \"\n",
    "    \"Your responses will be brief, to the point, concise and in compliance with the established format. \"\n",
    "    \"If a question falls outside the given context, you will simply output that you are sorry and you don't know about this. \"\n",
    "    \"The aim is to deliver professional, precise, and contextually relevant information pertaining to the context. \"\n",
    "    \"Use four sentences maximum. \"\n",
    "    \"\\nCONTEXT: {context}\"\n",
    ")\n",
    "\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt_template),\n",
    "        (\"placeholder\", \"{chat_history}\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['context', 'input'], optional_variables=['chat_history'], input_types={'chat_history': list[typing.Annotated[typing.Union[typing.Annotated[langchain_core.messages.ai.AIMessage, Tag(tag='ai')], typing.Annotated[langchain_core.messages.human.HumanMessage, Tag(tag='human')], typing.Annotated[langchain_core.messages.chat.ChatMessage, Tag(tag='chat')], typing.Annotated[langchain_core.messages.system.SystemMessage, Tag(tag='system')], typing.Annotated[langchain_core.messages.function.FunctionMessage, Tag(tag='function')], typing.Annotated[langchain_core.messages.tool.ToolMessage, Tag(tag='tool')], typing.Annotated[langchain_core.messages.ai.AIMessageChunk, Tag(tag='AIMessageChunk')], typing.Annotated[langchain_core.messages.human.HumanMessageChunk, Tag(tag='HumanMessageChunk')], typing.Annotated[langchain_core.messages.chat.ChatMessageChunk, Tag(tag='ChatMessageChunk')], typing.Annotated[langchain_core.messages.system.SystemMessageChunk, Tag(tag='SystemMessageChunk')], typing.Annotated[langchain_core.messages.function.FunctionMessageChunk, Tag(tag='FunctionMessageChunk')], typing.Annotated[langchain_core.messages.tool.ToolMessageChunk, Tag(tag='ToolMessageChunk')]], FieldInfo(annotation=NoneType, required=True, discriminator=Discriminator(discriminator=<function _get_type at 0x000001FCA98484A0>, custom_error_type=None, custom_error_message=None, custom_error_context=None))]]}, partial_variables={'chat_history': []}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template=\"As a Legal Assistant Chatbot specializing in legal queries, your primary objective is to provide accurate and concise information based on user queries. You will adhere strictly to the instructions provided, offering relevant context from the knowledge base while avoiding unnecessary details. Your responses will be brief, to the point, concise and in compliance with the established format. If a question falls outside the given context, you will simply output that you are sorry and you don't know about this. The aim is to deliver professional, precise, and contextually relevant information pertaining to the context. Use four sentences maximum. \\nCONTEXT: {context}\"), additional_kwargs={}), MessagesPlaceholder(variable_name='chat_history', optional=True), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User:  What are the conditions that must be fulfilled for a marriage to be solemnized in India?\n",
      "AI:  To solemnize a marriage in India, the following conditions must be fulfilled: \n",
      "\n",
      "The parties and three witnesses must sign a declaration in the presence of the Marriage Officer, and the declaration must be countersigned by the Marriage Officer. \n",
      "The marriage must be solemnized at the office of the Marriage Officer or at a place within a reasonable distance, with conditions and additional fees as prescribed. \n",
      "The marriage must be in a form chosen by the parties, but it must be complete and binding only if each party says to the other, in the presence of the Marriage Officer and witnesses, that they take the other as their lawful wife or husband.\n",
      "User:  Can you specify if there are any age requirements for the parties involved?\n",
      "AI:  According to the Special Marriage Act, 1954, the parties involved in a marriage must be at least 21 years of age for males and 18 years of age for females.\n"
     ]
    }
   ],
   "source": [
    "rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)\n",
    "\n",
    "\n",
    "chat_history = []\n",
    "\n",
    "question = \"What are the conditions that must be fulfilled for a marriage to be solemnized in India?\"\n",
    "ai_msg_1 = rag_chain.invoke({\"input\": question, \"chat_history\": chat_history})\n",
    "print(\"User: \", question)\n",
    "print(\"AI: \", ai_msg_1[\"answer\"])\n",
    "chat_history.extend(\n",
    "    [\n",
    "        HumanMessage(content=question),\n",
    "        AIMessage(content=ai_msg_1[\"answer\"]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "second_question = \"Can you specify if there are any age requirements for the parties involved?\"\n",
    "ai_msg_2 = rag_chain.invoke({\"input\": second_question, \"chat_history\": chat_history})\n",
    "\n",
    "print(\"User: \", second_question)\n",
    "print(\"AI: \", ai_msg_2[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
